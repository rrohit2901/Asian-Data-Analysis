{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUnSayFsEal4",
        "outputId": "298166cd-ed82-48a6-864f-cef3fcb47264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting colab on drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# Enter the folder name\n",
        "FOLDER_NAME = '/content/drive/My Drive/IndianData/Posts/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQVd__8dEal7"
      },
      "outputs": [],
      "source": [
        "# Extracting files from JSON\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "countF = 0\n",
        "countI = 0\n",
        "\n",
        "InstaPostList = []\n",
        "\n",
        "for folder in os.listdir(FOLDER_NAME):\n",
        "    if folder==\".ipynb_checkpoints\":\n",
        "        continue\n",
        "    foldername = FOLDER_NAME + folder\n",
        "    print(foldername)\n",
        "    for file in os.listdir(foldername):\n",
        "        if file==\".ipynb_checkpoints\":\n",
        "            continue\n",
        "        with open(foldername+\"/\"+file, \"r\") as f:\n",
        "            posts = f.readlines()\n",
        "            for post in posts:\n",
        "                post = json.loads(post)\n",
        "                try:\n",
        "                    for postF in post['result']['posts']:\n",
        "                        if postF['platform']==\"Facebook\":\n",
        "                            InstaPostList.append(postF)\n",
        "                except:\n",
        "                    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odJS06iMEal8"
      },
      "outputs": [],
      "source": [
        "# Removing duplicate posts\n",
        "\n",
        "IdDict = set()\n",
        "newList = []\n",
        "\n",
        "for post in InstaPostList:\n",
        "    if post['platformId'] in IdDict:\n",
        "        continue\n",
        "    else:\n",
        "        newList.append(post)\n",
        "        IdDict.add(post['platformId'])\n",
        "\n",
        "InstaPostList = newList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbYOg59LEal9"
      },
      "outputs": [],
      "source": [
        "# Monthly Post analysis\n",
        "from datetime import datetime\n",
        "\n",
        "MonthlyPost = dict()\n",
        "\n",
        "for post in InstaPostList:\n",
        "    month = datetime.strptime(post['date'], \"%Y-%m-%d %H:%M:%S\")\n",
        "    month = \"{}-{}-01\".format(month.year, month.month)\n",
        "    if month in MonthlyPost:\n",
        "        MonthlyPost[month] += 1\n",
        "    else:\n",
        "        MonthlyPost[month] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3ep4GO1Eal9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oc7aVZUEal-"
      },
      "outputs": [],
      "source": [
        "MonthlyPostDF = pd.DataFrame({\n",
        "    \"Month\" : [datetime.strptime(date, \"%Y-%m-%d\") for date in MonthlyPost.keys()],\n",
        "    \"Number of Posts\" : [val for val in MonthlyPost.values()]\n",
        "})\n",
        "plt.figure(figsize = (15,5))\n",
        "sns.lineplot(x=\"Month\", y=\"Number of Posts\", data=MonthlyPostDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i73yUUYEal_"
      },
      "outputs": [],
      "source": [
        "# Popular accounts/pages\n",
        "\n",
        "Accounts = dict()\n",
        "\n",
        "for post in InstaPostList:\n",
        "    if post['account']['name'] not in Accounts.keys():\n",
        "        Accounts[post['account']['name']] = {\n",
        "            \"subscriberCount\" : post['account']['subscriberCount'],\n",
        "            \"postCount\" : 0\n",
        "        }\n",
        "    Accounts[post['account']['name']]['subscriberCount'] = max(Accounts[post['account']['name']]['subscriberCount'], post['account']['subscriberCount'])\n",
        "    Accounts[post['account']['name']]['postCount'] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJDdfqwEamA"
      },
      "outputs": [],
      "source": [
        "AccountsSubs = {k: v for k, v in sorted(Accounts.items(), key=lambda item: item[1]['subscriberCount'], reverse=True)[:15]}\n",
        "\n",
        "for accn in AccountsSubs.keys():\n",
        "    print(\"Account - {}, Subscriber Count - {}, Post Count - {}\".format(accn, AccountsSubs[accn]['subscriberCount'], AccountsSubs[accn]['postCount']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5BZjMWkEamA"
      },
      "outputs": [],
      "source": [
        "AccountsPosts = {k: v for k, v in sorted(Accounts.items(), key=lambda item: item[1]['postCount'], reverse=True)[:15]}\n",
        "\n",
        "for accn in AccountsPosts.keys():\n",
        "    print(\"Account - {}, Subscriber Count - {}, Post Count - {}\".format(accn, AccountsPosts[accn]['subscriberCount'], AccountsPosts[accn]['postCount']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1tAETqVEamC"
      },
      "outputs": [],
      "source": [
        "# Overall word analysis\n",
        "\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re\n",
        "\n",
        "wordCount = dict()\n",
        "\n",
        "for post in InstaPostList:\n",
        "    try:\n",
        "        text = post['message']\n",
        "    except:\n",
        "        try:\n",
        "            text = post['description']\n",
        "        except:\n",
        "            continue \n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = text.replace('\\n',\" \")\n",
        "    text = text.replace('\\t',\" \")\n",
        "    text = text.lower()\n",
        "    res = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    res = remove_stopwords(res)\n",
        "    words = res.split()\n",
        "    for word in words:\n",
        "        if(len(word) <= 1):\n",
        "            continue\n",
        "        if word not in wordCount.keys():\n",
        "            wordCount[word] = 0\n",
        "        wordCount[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJLr1SNAEamC"
      },
      "outputs": [],
      "source": [
        "wordCountSorted = sorted(wordCount, key=wordCount.get, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "443lanpREamD"
      },
      "outputs": [],
      "source": [
        "wordCountSorted[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwBXEQ4ZEamD"
      },
      "outputs": [],
      "source": [
        "# Word cloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "txt = \"\"\n",
        "for word in wordCount.keys():\n",
        "    for i in range(wordCount[word]):\n",
        "        txt += word + \" \"\n",
        "\n",
        "plt.figure(figsize = (5,5))\n",
        "word_cloud = WordCloud(collocations = False, background_color = 'white', max_words=100).generate(txt)\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XIx4EfzEamE"
      },
      "outputs": [],
      "source": [
        "# Hashtag freuqency\n",
        "import re\n",
        "\n",
        "all_hashtags = dict()\n",
        "hashtag_re = re.compile(\"#\\w+ \")\n",
        "\n",
        "for post in InstaPostList:\n",
        "    try:\n",
        "        line = post['message']\n",
        "    except:\n",
        "        try:\n",
        "            line = post['description']\n",
        "        except:\n",
        "            continue \n",
        "    try:\n",
        "        line = line.strip();\n",
        "        line = line.lower();\n",
        "    except:\n",
        "        continue;\n",
        "    tweet = line;\n",
        "    hashtags = re.findall(hashtag_re,tweet);\n",
        "    if(len(hashtags)>0):\n",
        "        for hashtag in hashtags:\n",
        "            hashtag = hashtag.strip();\n",
        "            if(len(hashtag)<3):\n",
        "                continue;\n",
        "            if hashtag in all_hashtags.keys():\n",
        "                all_hashtags[hashtag] += 1\n",
        "            else:\n",
        "                all_hashtags[hashtag] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wugiu2-QEamE"
      },
      "outputs": [],
      "source": [
        "all_hashtags = {k:v for k,v in sorted(all_hashtags.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo91qjmkEamE"
      },
      "outputs": [],
      "source": [
        "all_hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPDvS1x8EamF"
      },
      "outputs": [],
      "source": [
        "# URL Frequency\n",
        "URLs = dict()\n",
        "\n",
        "for post in InstaPostList:\n",
        "  try:\n",
        "    post = post['expandedLinks']\n",
        "\n",
        "    for link in post:\n",
        "      url = link['original']\n",
        "      if url not in URLs.keys():\n",
        "        URLs[url] = 0\n",
        "      URLs[url] += 1\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s14ZlLT_EamF"
      },
      "outputs": [],
      "source": [
        "URLs = {k:v for k,v in sorted(URLs.items(), key=lambda item:item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwrL1ImfEamF"
      },
      "outputs": [],
      "source": [
        "URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDNsZ5z6EamF"
      },
      "outputs": [],
      "source": [
        "# Reach analysis\n",
        "shareCount = []\n",
        "CommentCount = []\n",
        "loveCount = []\n",
        "wowCount = []\n",
        "hahaCount = []\n",
        "sadCount = []\n",
        "angryCount = []\n",
        "thankfulCount = []\n",
        "careCount = []\n",
        "\n",
        "\n",
        "for post in InstaPostList:\n",
        "  try:\n",
        "    shareCount.append(post['statistics']['actual']['shareCount'])\n",
        "    loveCount.append(post['statistics']['actual']['loveCount'])\n",
        "    wowCount.append(post['statistics']['actual']['wowCount'])\n",
        "    hahaCount.append(post['statistics']['actual']['hahaCount'])\n",
        "    sadCount.append(post['statistics']['actual']['sadCount'])\n",
        "    angryCount.append(post['statistics']['actual']['angryCount'])\n",
        "    thankfulCount.append(post['statistics']['actual']['thankfulCount'])\n",
        "    careCount.append(post['statistics']['actual']['careCount'])\n",
        "    CommentCount.append(post['statistics']['actual']['commentCount'])\n",
        "  except:\n",
        "    continue\n",
        "    \n",
        "import statistics \n",
        "\n",
        "print(\"Share count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(shareCount), statistics.median(shareCount), statistics.stdev(shareCount), max(shareCount)))\n",
        "print(\"Comment count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(CommentCount), statistics.median(CommentCount), statistics.stdev(CommentCount), max(CommentCount)))\n",
        "print(\"Love count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(loveCount), statistics.median(loveCount), statistics.stdev(loveCount), max(loveCount)))\n",
        "print(\"Wow count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(wowCount), statistics.median(wowCount), statistics.stdev(wowCount), max(wowCount)))\n",
        "print(\"Sad count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(hahaCount), statistics.median(hahaCount), statistics.stdev(hahaCount), max(hahaCount)))\n",
        "print(\"Angry count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(sadCount), statistics.median(sadCount), statistics.stdev(sadCount), max(sadCount)))\n",
        "print(\"Thankful count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(angryCount), statistics.median(angryCount), statistics.stdev(angryCount), max(angryCount)))\n",
        "print(\"Care count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(thankfulCount), statistics.median(thankfulCount), statistics.stdev(thankfulCount), max(thankfulCount)))\n",
        "print(\"Commend count - Mean = {}, Median = {}, Standard Deviation = {}, Max = {}\".format(statistics.mean(careCount), statistics.median(careCount), statistics.stdev(careCount), max(careCount)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk1pWB4JEamG"
      },
      "outputs": [],
      "source": [
        "# Word vector analysis\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBTSCyoKEamG"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "for post in InstaPostList:\n",
        "    try:\n",
        "        text = post['message']\n",
        "    except:\n",
        "        try:\n",
        "            text = post['description']\n",
        "        except:\n",
        "            continue \n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = text.replace('\\n',\" \")\n",
        "    text = text.replace('\\t',\" \")\n",
        "    text = text.lower()\n",
        "    res = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    res = remove_stopwords(res)\n",
        "    txt = []\n",
        "    words = res.split()\n",
        "    for word in words:\n",
        "        if(len(word) <= 1):\n",
        "            continue\n",
        "        txt.append(word)\n",
        "    sentences.append(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9K578VFEamG"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(sentences=sentences, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhwBJusmEamH"
      },
      "outputs": [],
      "source": [
        "IntialWordList = [\"asian\", \"virus\", \"coronavirus\", \"chinese\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gyYWXWuEamH"
      },
      "outputs": [],
      "source": [
        "for word in IntialWordList:\n",
        "    sims = model.wv.most_similar(word, topn=10)\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"Word similar to {}: \".format(word))\n",
        "    for word, score in sims:\n",
        "        print(\"Word - {}, Similarity score - {}\".format(word, score))\n",
        "    print(\"----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb1KTrelEamH"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import community.community_louvain as community_louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxHqAVd1EamH"
      },
      "outputs": [],
      "source": [
        "!pip3 install python-louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsLYNwHMEamI"
      },
      "outputs": [],
      "source": [
        "numLevel = 2\n",
        "thres = 0.5\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "def makeGraph(words, lev=2):\n",
        "    for word in words:\n",
        "        if word not in G.nodes():\n",
        "            G.add_node(word)\n",
        "            sims = model.wv.most_similar(word, topn=10)\n",
        "            newWordList = []\n",
        "            for wordN,sim in sims:\n",
        "                if sim>thres:\n",
        "                    newWordList.append(wordN)\n",
        "            if lev>0:\n",
        "                makeGraph(newWordList, lev-1)\n",
        "                for wordN, sim in sims:\n",
        "                    if sim>thres:\n",
        "                        G.add_edge(word, wordN, weight=sim)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4hS15MHEamI"
      },
      "outputs": [],
      "source": [
        "makeGraph(IntialWordList)\n",
        "G = G.to_undirected()\n",
        "partitionLev2 = community_louvain.best_partition(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFOars8eEamI"
      },
      "outputs": [],
      "source": [
        "ns = []\n",
        "for node in G.nodes():\n",
        "    ns.append(wordCount[node])\n",
        "mns = statistics.mean(ns)\n",
        "for i in range(len(ns)):\n",
        "    ns[i] /= mns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCfzQ_5JEamI"
      },
      "outputs": [],
      "source": [
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize = (25,25))\n",
        "# color the nodes according to their partition\n",
        "cmap = cm.get_cmap('viridis', max(partitionLev2.values()) + 1)\n",
        "nx.draw_networkx_nodes(G, pos, partitionLev2.keys(), node_size=[100*x for x in ns], cmap=cmap, node_color=list(partitionLev2.values()))\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10, horizontalalignment=\"right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmONXUC9EamI"
      },
      "outputs": [],
      "source": [
        "# Topic modelling\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "\n",
        "data_words = []\n",
        "for post in InstaPostList:\n",
        "    try:\n",
        "        text = post['message']\n",
        "    except:\n",
        "        try:\n",
        "            text = post['description']\n",
        "        except:\n",
        "            continue \n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    res = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    res = remove_stopwords(res)\n",
        "    data_word = simple_preprocess(res)\n",
        "    data_word = [word for word in data_word if word not in stop_words]\n",
        "    data_words.append(data_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i4OtJc7EamJ"
      },
      "outputs": [],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "texts = data_words\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzAW21MwEamJ"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import warnings\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "# Build LDA model\n",
        "warnings.simplefilter(\"ignore\")\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                    id2word=id2word,\n",
        "                                    num_topics=num_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_CXJQpaEamJ"
      },
      "outputs": [],
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOELHtfvEamJ"
      },
      "outputs": [],
      "source": [
        "# Hashtag co-ocurrence analysis\n",
        "\n",
        "currhashs = [k for (k,v) in all_hashtags.items() if v>200]\n",
        "\n",
        "CorrCount = dict()\n",
        "for hashtag in currhashs:\n",
        "    CorrCount[hashtag] = dict()\n",
        "    for hashtag2 in currhashs:\n",
        "        if hashtag!=hashtag2:\n",
        "            CorrCount[hashtag][hashtag2] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVrZEn99EamJ"
      },
      "outputs": [],
      "source": [
        "hashtag_re = re.compile(\"#\\w+ \")\n",
        "\n",
        "for post in InstaPostList:\n",
        "    try:\n",
        "        line = post['message']\n",
        "    except:\n",
        "        try:\n",
        "            line = post['description']\n",
        "        except:\n",
        "            continue \n",
        "    try:\n",
        "        line = line.strip();\n",
        "        line = line.lower();\n",
        "    except:\n",
        "        continue;\n",
        "    tweet = line;\n",
        "    hashtags = re.findall(hashtag_re,tweet);\n",
        "    if(len(hashtags)>0):\n",
        "        for hashtag in hashtags:\n",
        "            hashorig = hashtag\n",
        "            hashtag = hashtag.strip();\n",
        "            if len(hashtag)<3:\n",
        "                hashtags.remove(hashorig)\n",
        "        for hashtag1 in hashtags:\n",
        "            for hashtag2 in hashtags:\n",
        "                hashtag1 = hashtag1.strip()\n",
        "                hashtag2 = hashtag2.strip()\n",
        "                if hashtag1!=hashtag2 and (hashtag1 in currhashs) and (hashtag2 in currhashs):\n",
        "                    CorrCount[hashtag1][hashtag2] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMOiOQA3EamK"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import community.community_louvain as community_louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBPCYH3REamK"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "def makeGraph():\n",
        "    for hashtag in currhashs:\n",
        "        G.add_node(hashtag)\n",
        "    for hashtag1 in currhashs:\n",
        "        for hashtag2 in currhashs:\n",
        "            if hashtag1!=hashtag2:\n",
        "                if CorrCount[hashtag1][hashtag2]!=0:\n",
        "                    weight = float(float(CorrCount[hashtag1][hashtag2]) / float(all_hashtags[hashtag1] * all_hashtags[hashtag2]))\n",
        "                    G.add_edge(hashtag1, hashtag2, weight = weight)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF5_MfxJEamK"
      },
      "outputs": [],
      "source": [
        "makeGraph()\n",
        "G = G.to_undirected()\n",
        "partitionLev2 = community_louvain.best_partition(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d7Yx_lQEamK"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "ns = []\n",
        "for node in G.nodes():\n",
        "    ns.append(all_hashtags[node])\n",
        "mns = statistics.mean(ns)\n",
        "for i in range(len(ns)):\n",
        "    ns[i] /= mns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAE9lCG2EamK"
      },
      "outputs": [],
      "source": [
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize = (30,30))\n",
        "# color the nodes according to their partition\n",
        "cmap = cm.get_cmap('viridis', max(partitionLev2.values()) + 1)\n",
        "nx.draw_networkx_nodes(G, pos, partitionLev2.keys(), node_size=[100*x for x in ns], cmap=cmap, node_color=list(partitionLev2.values()))\n",
        "# nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10, horizontalalignment=\"right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-bae6hOEamL"
      },
      "outputs": [],
      "source": [
        "# Post language analysis\n",
        "\n",
        "languageCount = dict()\n",
        "\n",
        "for post in InstaPostList:\n",
        "    langCode = post[\"languageCode\"]\n",
        "    if langCode not in languageCount.keys():\n",
        "        languageCount[langCode] = 0\n",
        "    languageCount[langCode]+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmq7JfkdEamL"
      },
      "outputs": [],
      "source": [
        "languageCount = {k:v for k,v in sorted(languageCount.items(), key=lambda item:item[1], reverse=True)}\n",
        "\n",
        "languageCount"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Facebook.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}